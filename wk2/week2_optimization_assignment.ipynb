{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tobig's 14기 2주차 Optimization 과제\n",
    "### Made by 이지용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent 구현하기\n",
    "\n",
    "### 1) \"...\" 표시되어 있는 빈 칸을 채워주세요  \n",
    "### 2) 강의내용과 코드에 대해 공부한 내용을 적어서 과제를 채워주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기\n",
    "### 데이터셋을 train/test로 나눠주는 메소드  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size=0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling  \n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
    "똑같이 X_test에다 fit하면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47943121, 0.77132011, 0.21115225])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = \"1/(1+e^{-x}))\" $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)) :\n",
    "        z += X[i]*parameters[i]\n",
    "    p =  1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* e^(-x)에서 x는 $\\sum Bx$ 이기 때문에 parameter의 개수만큼 반복하여 z에 더해주고 p에서 그 확률값을 받아 return한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8026847080785801"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "로지스틱 회귀의 목적함수를 작성해주세요\n",
    "## $l(p) =\"\\sum(ylog(p)+(1-y)log(1-p))\"$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_i(X, y, parameters) :\n",
    "    p = logistic(X, parameters)                       # 위에서 작성한 함수를 활용하세요\n",
    "    loss = y*np.log(p)+(1-y)*np.log(1-p)\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(X_set, y_set, parameters) :\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i, :]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += cross_entropy_i(X, y, parameters)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cross_entropy_i로 목적함수의 식을 구현해주고 cross_entropy 안에서 반복해주면서 값을 다 더한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.78312829447642"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(X_test, y_test, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of Cross Entropy\n",
    "\n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)= \"-\\sum(y-p)x\"$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_entropy를 theta_j에 대해 미분한 값을 구하는 함수\n",
    "def get_gradient_ij_cross_entropy(X, y, parameters, j):\n",
    "    p = logistic(X, parameters)\n",
    "    gradient = -(y-p)*X[j]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 반대방향으로 가야하기때문에 미분한것에 -가 있는것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07617525971642503"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij_cross_entropy(X_train.iloc[0, :], y_train.iloc[0], parameters, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent  \n",
    "\n",
    "Batch Gradient Descent : $\"x^{k}=x^{k-1}−tk⋅\\sum_{i=1}^{m}fi(x^{k-1}),k=1,2,3,..\"$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습 한번에 모든 데이터셋에 대해 기울기를 구한다\n",
    "* 방향에 대해서는 정확할 수 있지만 계산량이 매우많다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients_bgd(X_train, y_train, parameters) :\n",
    "    gradients = [0 for i in range(len(parameters))]\n",
    "    \n",
    "    for i in range(X_train.shape[0]):\n",
    "        X = X_train.iloc[i, :]\n",
    "        y = y_train.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij_cross_entropy(X, y, parameters, j)\n",
    "            \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gradients에 값을 parameter의 개수만큼 0으로 선언해준다\n",
    "* gradients에 값에 미분한 점들로 더해준다(위에서 구한 get_gradient_ij_cross_entropy 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47.45687076818456, 11.123177010732196, 38.3697934212434]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients_bgd = get_gradients_bgd(X_train, y_train, parameters)\n",
    "gradients_bgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent  \n",
    "\n",
    "Stochastic Gradient Descent : $\"x^{k+1}=x^{k}−tk⋅∇fik(x^{k−1}),ik \\in \\{1,2,...,m\\}\"$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습 한번에 임의의 데이터에 대해서만 기울기를 구한다\n",
    "* 데이터를 넣기전에 무작위하게 섞어주고 차례대로 뽑는게 아니라 임의로 뽑아서 gradient를 구하는방법이다. \n",
    "* Batch Gradient Descent와는 다르게 하나의 데이터를 이용하여 업데이트하기 떄문에 속도는 빠를수 았으나 정확하지 않을 수 있다.\n",
    "* 그렇기 때문에 Batch와 Stochastic을 합쳐놓은듯한 Mini-Batch를 많이 사용한다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients_sgd(X_train, y, parameters) :\n",
    "    gradients = [0 for i in range(len(parameters))]\n",
    "    r = int(random.random()*(X_train.shape[0]))\n",
    "    X = X_train.iloc[r, :]\n",
    "    y = y_train.iloc[r]\n",
    "        \n",
    "    for j in range(len(parameters)):\n",
    "        gradients[j] =  get_gradient_ij_cross_entropy(X, y, parameters, j)\n",
    "        \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27907769096593915, -0.45469495488095796, -0.2270371795417179]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients_sgd = get_gradients_sgd(X_train, y_train, parameters)\n",
    "gradients_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate) :\n",
    "    for i in range(len(parameters)) :\n",
    "        gradients[i] *= learning_rate\n",
    "    parameters -= gradients\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gradients에 학습률만큼 모수를 업데이트 시켜준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0048625 ,  0.66008834, -0.17254569])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_parameters(parameters, gradients_bgd, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent  \n",
    "\n",
    "위에서 작성한 함수들을 조합해서 Gradient Descent를 진행하는 함수를 완성해주세요\n",
    "\n",
    "learning_rate = \"학습률\"  \n",
    "max_iter = \"최대 반복횟수\"  \n",
    "tolerance = \"변화율이 tolerance보다 작으면 학습을 그만한다\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate=0.01, max_iter=100000, tolerance=0.0001, optimizer=\"bgd\") :\n",
    "    count = 1\n",
    "    point = 100 if optimizer == \"bgd\" else 10000\n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.array([random.random() for i in range(N)])\n",
    "    gradients = [0 for i in range(N)]\n",
    "    loss = cross_entropy(X_train, y_train, parameters)\n",
    "    \n",
    "    while count < max_iter :\n",
    "        \n",
    "        if optimizer == \"bgd\" :\n",
    "            gradients = get_gradients_bgd(X_train, y_train, parameters)\n",
    "        elif optimizer == \"sgd\" :\n",
    "            gradients = get_gradients_sgd(X_train, y_train, parameters)\n",
    "            # loss, 중단 확인\n",
    "        if count%point == 0 :\n",
    "            new_loss = cross_entropy(X_train, y_train, parameters)\n",
    "            print(count, \"loss: \",new_loss, \"params: \", parameters, \"gradients: \", gradients)\n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss-loss) < tolerance*len(y_train) :\n",
    "                break\n",
    "            loss = new_loss\n",
    "                \n",
    "            \n",
    "                \n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        count += 1\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loss:  45.38015700324572 params:  [-1.62451963  3.47646664 -3.30263354] gradients:  [0.27622568162035926, -0.9089056475351965, 0.8495535155928922]\n",
      "200 loss:  44.80016573469691 params:  [-1.78213122  3.99162367 -3.78159499] gradients:  [0.0831963192569357, -0.27034643851353923, 0.25001806976136465]\n",
      "300 loss:  44.74021028343032 params:  [-1.83414216  4.16037091 -3.93739771] gradients:  [0.029956146053548974, -0.09704300445760279, 0.0894502013742747]\n",
      "400 loss:  44.73212098733444 params:  [-1.85338472  4.22267447 -3.99479258] gradients:  [0.011387147404068182, -0.036849762270933485, 0.03392605700446663]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.85338472,  4.22267447, -3.99479258])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train)\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning\n",
    "\n",
    "Hyper Parameter들을 매번 다르게 해서 학습을 진행해 보세요. 다른 점들을 발견할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gradient descent 특징은 learning rate, iter 횟수 등과 같은 hyper parater 학습을 통해 얻는것이 아니라 사람이 직접 넣어주는 값이라고 한다. (모수 parameter는 학습을 통해서 얻어진다)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loss:  46.54438391412418 params:  [-1.43493838  3.01140181 -2.91592548] gradients:  [0.004054417735083267, -0.0053056118784497565, 0.000178290673859503]\n",
      "20000 loss:  45.03954202337346 params:  [-1.71109631  3.70273559 -3.52247166] gradients:  [0.28413741082912586, 0.2862403819271171, 0.23740078634082887]\n",
      "30000 loss:  44.979189479329705 params:  [-1.79812758  3.88931191 -3.84217575] gradients:  [0.006815579658324907, -0.007947490943932916, -0.002397689373763393]\n",
      "40000 loss:  44.87452961934882 params:  [-1.89681346  4.11813949 -4.034833  ] gradients:  [0.0957263666859769, 0.014575511906823565, 0.02315231371248387]\n",
      "50000 loss:  44.75806371677091 params:  [-1.85601122  4.28758581 -3.99079496] gradients:  [0.0004795226480606663, -0.0006275034354554744, 2.108673097836912e-05]\n",
      "60000 loss:  44.81538916495611 params:  [-1.76405043  4.25419405 -4.05874088] gradients:  [-0.37097497088180875, 0.18144089003230005, 0.3996785372642768]\n",
      "70000 loss:  44.7809926263097 params:  [-1.816949    4.18391604 -4.03521357] gradients:  [0.04473862473347224, -0.017099085558697127, -0.003934710270358671]\n",
      "80000 loss:  44.810869382919755 params:  [-1.80198561  4.33751146 -4.02052487] gradients:  [0.001048596953511964, -0.0013721941881119572, -0.0001613900549537009]\n",
      "90000 loss:  44.856678114323216 params:  [-1.92129066  4.33845912 -3.97475378] gradients:  [0.005951042497438805, -0.006303252947234981, -0.002093549202597138]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.89829822,  4.24538887, -4.05240859])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, learning_rate=0.01, max_iter=100000, tolerance=0.0001, optimizer=\"sgd\")\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38,  2],\n",
       "       [ 1,  9]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.94\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_predict) #(tp+tn)/(tp+fn+fp+tn)\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정확도가 상당히 높다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_predict) #(tp)/(tp+fp)\n",
    "recall = recall_score(y_test,y_predict) #(tp)/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "f1_score = f1_score(y_test, y_predict) #2*(precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8181818181818182\n",
      "recall:  0.9\n",
      "specificity:  0.95\n",
      "f1_score:  0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "print(\"precision: \", precision)\n",
    "print(\"recall: \", recall)\n",
    "print(\"specificity: \", specificity)\n",
    "print(\"f1_score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 참고한 자료 <br>\n",
    "https://www.edwith.org/aipython/lecture/24518/ <br>\n",
    "https://deepapple.tistory.com/7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
