{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer - Attention Is All You Need 논문리뷰\n",
    "투빅스 14기 한유진"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract\n",
    "encoder-decoder architecture에서 가장 일반적으로 사용되는 recurrence and convolutions를 제거하고 attention 매커니즘만을 활용하여 새롭고 단순한 network architecture인 Transformer를 제안하였다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "RNN, LSTM, GRU는 sequence modeling and transduction problems에서 많은 성과를 이루어냈다. \n",
    "* Recurrent models<br>\n",
    "input과 output 순서의 위치에 따라 계산을 하기 떄문에 병렬처리가 불가 -> 최근연구에서 연산 효율을 향상시켰지만 여전히 근본적인 순차적계산의 문제는 남아있다 \n",
    "* Attention mechanism<br>\n",
    "Input or output sequence의 거리제약 받지않는다 -> 그러나 일부 경우를 제외하고는 recurrent network와 함께 사용된다\n",
    "* Transformer<br>\n",
    "recurrence 사용X -> attention mechanism만 사용 -> 병렬처리 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture\n",
    "* Transformer의 전체적인 architecture는 encoder와 decoder에 stacked self-attention,point-wise,fully connected layers 을 사용한다\n",
    "##### Encoder\n",
    "* N = 6개의 layers로 구성\n",
    "* 2개의 sub-layers도 있다. 첫번째는 \"multi-head self-attention mechanism\", 두번째는 \"position-wise fully connected feed-forward network\"\n",
    "* 두개의 sub-layers 주위에 residual connection을 주고 layer normalization도 진행한다\n",
    "* residual connections을 쉽게하기위해 dmodel = 512로 임베딩한다\n",
    "##### Decoder\n",
    "* Encoder와 마찬가지로  N = 6개의 layers와 2개의 sub-layers를 가지지만 여기에 multi-head self-attention을 하는 3번째 sub-layer가 추가된다\n",
    "* 또한 sub-layers 주위에 residual connection을 주고 layer normalization도 진행한다 \n",
    "* masking은 출력 임베딩이 한 위치만큼 오프셋된다는 사실과 결합되어 위치 i에 대한 예측은 i보다 작은 위치에서 알려진 출력에만 의존 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention\n",
    "* Attention function은 query와 키-값쌍 집합을 출력에 mappint하는것으로 설명할 수있다(query, key,values,output은 모두 벡터, output은 가중합계로 계산) \n",
    "##### Sclaed Dot-Product Attention\n",
    "* Attention(Q, K, V ) = softmax(QKT/√dk)V \n",
    "##### Multi-Head Attention\n",
    "* dmodel 차원의 keys, valaues, queries로 single attention function을 수행하는 대신에 queries, keys. values를 각각 dk,dk,dv 차원에 대해 학습하여 h번 계산(백터의 크기가 줄어들고 병렬처리가 가능)\n",
    "* MultiHead(Q, K, V ) = Concat(head1, ..., headh)Wo\n",
    "##### Application of Attention in our Model\n",
    "Transformer는 3가지 방법으로 multi-head attention을 사용한다\n",
    "1. 'encoder-decoder attention' layer에서 query는 이전 decoder layer로부터 오고, keys,values는 encoder의 output으로부터온다 -> every position in the decoder to attend over all positions in the input sequence(seq2seq방식과 같음)\n",
    "2. encoder는 self-attention layer를 포함한다. encoder의 각 position은 encoder의 이전 layer에 모든 position에 참여할 수있다.\n",
    "3. 비슷한 방법으로, decoder의 self-attention을 사용하면 decoder의 각 위치가 해당 위치까지 decoder의 모든 위치에 집중할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position-wise Feed-Forward Networks\n",
    "* encoder, decoder의 layer에는 attention sub-layers 말고도 fully connected feed-forward network도 가지고 있다. -> ReLU 활성화가 중간에있는 두 개의 선형 변환으로 구성된다.\n",
    "\n",
    "#### Embeddings and Softmax\n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
    "tokens and output tokens to vectors of dimension dmodel. \n",
    "\n",
    "#### Positional Encoding\n",
    "* 논문의 모델은 no recurrence and no convolution -> 마지막에 'positional encodings'를 추가해줘야한다(sequence의 위치관련 정보를 주기위해)\n",
    "* P E(pos,2i) = sin(pos/100002i/dmodel), P E(pos,2i+1) = cos(pos/100002i/dmodel) 이 두개의 식을 사용한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Self-Attention\n",
    "1. 복잡한 연산에서 recurrent layers보다 빠르다(n이 d보다 작으니까)\n",
    "2. 병렬화 할 수있는 계산의 양이 늘어난다\n",
    "3. network에서 long-range dependencies간의 경로 길이이다(input위치-output위치 길이가 짧을수록 long-range dependencies학습이 쉬워진다)\n",
    "4. 추가로, 더많은 해석이 가능한 모델을 만들 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "* Transformer는 recurrent or convolutional layers를 기반으로한 architecture보다 훨씬 빠른 훈련속도를 보였다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
